# About-Diffusers

For the math of transformer models, materials can be found easily in many places. Here, the article dysmystified some important topics in transformer models, https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/.

Some articles were listed here for reference, their content was primaryly about the arithmetic issues related to transformer models. 

As for the arithmetic intensiry required to carry out transformer inference, some details have been presented in an article, https://kipp.ly/transformer-inference-arithmetic/. Besides the discussion on FLOPs, extra analysis was made on the memory costs as well. In addition, reference can be found in another artile, https://www.adamcasson.com/posts/transformer-flops.

In analysis, the roof-line model has always been used, and in https://www.baseten.co/blog/llm-transformer-inference-guide/, there is some analysis work on LLM inference and performance. Furthermore, benchmarking tests were conducted to figure out the inference performance of LLM models, as shown in https://www.baseten.co/blog/benchmarking-fast-mistral-7b-inference/. 
